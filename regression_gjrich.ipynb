{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72cd216e",
   "metadata": {},
   "source": [
    "# Final Project: Regression Analysis\n",
    "\n",
    "> Submission: GitHub Repository with Jupyter Notebook and Peer Review\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "Businesses and organizations often need to understand the relationships between different factors to make better decisions.\n",
    "For example, a company may want to predict the fuel efficiency of a car based on its weight and engine size or estimate home prices based on square footage and location.\n",
    "Regression analysis helps identify and quantify these relationships between numerical features, providing insights that can be used for forecasting and decision-making.\n",
    "\n",
    "This project demonstrates your ability to apply regression modeling techniques to a real-world dataset. You will:\n",
    "- Load and explore a dataset.\n",
    "- Choose and justify features for predicting a target variable.\n",
    "- Train a regression model and evaluate performance.\n",
    "- Compare multiple regression approaches.\n",
    "- Document your work in a structured Jupyter Notebook.\n",
    "- Conduct a peer review of a classmate's project.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Options\n",
    "Select one dataset from the list below. If you get good results, you can try the process on a suitable dataset of your own. \n",
    "Suitable datasets contain **numerical features** and a **numerical target variable** for regression.\n",
    "\n",
    "1. Auto MPG Dataset (Predict fuel efficiency based on engine specs and weight)\n",
    "   - [UCI Auto MPG Dataset](https://archive-beta.ics.uci.edu/ml/datasets/auto+mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7d99e",
   "metadata": {},
   "source": [
    "## Section 1. Import and Inspect the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba5f7c",
   "metadata": {},
   "source": [
    "### 1.0 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import ks_2samp, wasserstein_distance, energy_distance\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "#using this variable allows us to \n",
    "state_setter=747"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371e416",
   "metadata": {},
   "source": [
    "### 1.1 Load the dataset and display the first 10 rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797697b",
   "metadata": {},
   "source": [
    "I used utils/convert_to_csv.py to convert our original auto-mpg.data file to a csv for easy consumption by Pandas. Pandas also had trouble with the data set due to some corrupted values, so convert_to_csv converts any mismatched values in auto-mpg.data to blanks when generating auto-mpg.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e56cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/auto-mpg.csv')\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(\"First 10 rows of the dataset:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de911c6",
   "metadata": {},
   "source": [
    "### 1.2 Check for missing values and display summary statistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35154594",
   "metadata": {},
   "source": [
    "Horsepower is missing 6 values. We will address this in section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9294af",
   "metadata": {},
   "source": [
    "Some summary statistics for the info. note that unique, top, and freq are generated for categorical features whereas mean, std, and min are generated for numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check unique values in categorical columns\n",
    "print(\"\\nUnique values in 'origin' column:\")\n",
    "print(df['origin'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686e017",
   "metadata": {},
   "source": [
    "We have mostly numerical columns with a couple string columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc3021",
   "metadata": {},
   "source": [
    "### Reflection 1: What do you notice about the dataset? Are there any data issues?\n",
    "\n",
    "The 'origin' column contains only integers (1, 2, and 3), but likely represents categorical information about manufacturing locations that should be properly encoded. The dataset spans model years from the 1970s to early 1980s based on the 'model_year' column, making this a historical dataset that might not reflect current automotive technology. These issues will need to be resolved through appropriate data cleaning and transformation steps before proceeding with modeling. The engine is misfiring on this dataset, but with some fine-tuning, we'll have it purring in no time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992c999",
   "metadata": {},
   "source": [
    "## Section 2. Data Exploration and Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8048d1",
   "metadata": {},
   "source": [
    "### 2.1 Handle missing values and clean data\n",
    "First, let's examine our missing values more closely and create a plan to handle them. We already identified that the 'horsepower' column has 6 missing values, and they appear to be represented as '?' characters. Let's double check that all got taken care of with our earlier work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff50a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for mismatched data types in the dataframe\n",
    "def check_data_type_mismatches(df):\n",
    "    print(\"Checking for data type mismatches in each column...\")\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    mismatches = {}\n",
    "    \n",
    "    # Check each column\n",
    "    for column in df.columns:\n",
    "        # Get the data type of the column\n",
    "        dtype = df[column].dtype\n",
    "        \n",
    "        # Check for mismatches based on the data type\n",
    "        if dtype == 'int64' or dtype == 'float64':\n",
    "            # For numeric columns, check for non-numeric values\n",
    "            non_numeric_count = 0\n",
    "            non_numeric_indices = []\n",
    "            \n",
    "            for i, value in enumerate(df[column]):\n",
    "                # Try to convert to float to see if it's numeric\n",
    "                try:\n",
    "                    float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    # If conversion fails, it's not numeric\n",
    "                    non_numeric_count += 1\n",
    "                    if non_numeric_count <= 5:  # Limit to first 5 examples\n",
    "                        non_numeric_indices.append(i)\n",
    "            \n",
    "            if non_numeric_count > 0:\n",
    "                mismatches[column] = {\n",
    "                    'expected_type': dtype,\n",
    "                    'mismatch_count': non_numeric_count,\n",
    "                    'example_indices': non_numeric_indices\n",
    "                }\n",
    "    \n",
    "    # Print results\n",
    "    if mismatches:\n",
    "        print(\"\\nMismatched data types found:\")\n",
    "        for column, info in mismatches.items():\n",
    "            print(f\"\\nColumn: {column}\")\n",
    "            print(f\"Expected type: {info['expected_type']}\")\n",
    "            print(f\"Mismatches found: {info['mismatch_count']}\")\n",
    "            \n",
    "            # Print examples\n",
    "            print(\"Examples of mismatched values:\")\n",
    "            for idx in info['example_indices']:\n",
    "                print(f\"  Row {idx}: '{df.loc[idx, column]}'\")\n",
    "    else:\n",
    "        print(\"\\nNo data type mismatches found!\")\n",
    "    \n",
    "    return mismatches\n",
    "\n",
    "# Run the function on our dataframe\n",
    "mismatches = check_data_type_mismatches(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e94de",
   "metadata": {},
   "source": [
    "Looks good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05557718",
   "metadata": {},
   "source": [
    "#### 2.1.1 Impute or drop missing values\n",
    "Now that we've properly identified the missing values, we'll impute them using the median value of the horsepower column. This is a reasonable approach for this small number (6 out of ~400) of missing values in a numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imputer for the horsepower column\n",
    "horsepower_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Fit the imputer on the horsepower data and transform it\n",
    "df['horsepower'] = horsepower_imputer.fit_transform(df[['horsepower']])\n",
    "\n",
    "# Verify that missing values have been imputed\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691eddd9",
   "metadata": {},
   "source": [
    "Worked well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6683ed4c",
   "metadata": {},
   "source": [
    "#### 2.1.2 Remove or transform outliers\n",
    "Let's identify potential outliers in our numerical columns using boxplots. This will help us determine if any data points are significantly outside the normal range and might need to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for all numerical columns to identify outliers\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Select only numerical columns\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create boxplots for each numerical column\n",
    "for i, column in enumerate(numerical_cols):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(x=df[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate the IQR and identify outliers for each numerical column\n",
    "for column in numerical_cols:\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n",
    "    \n",
    "    if outliers > 0:\n",
    "        print(f\"Column '{column}' has {outliers} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705bd54",
   "metadata": {},
   "source": [
    "#### 2.1.3 Convert categorical data to numerical format using encoding\n",
    "The 'origin' column is currently represented as integers (1, 2, 3) but it's actually a categorical feature representing the car's manufacturing region. \n",
    "\n",
    "By examining a few example rows, we can see that 1 is america, 2 is Europe, and 3 is Asia. Here are some example rows evidencing that:\n",
    "\n",
    "| mpg | cylinders | displacement | horsepower | weight | acceleration | model_year | origin | car_name |\n",
    "|-----|-----------|--------------|------------|--------|--------------|------------|--------|----------|\n",
    "| 18.0 | 8 | 307.0 | 130.0 | 3504.0 | 12.0 | 70 | 1 | \"chevrolet chevelle malibu\" |\n",
    "| 15.0 | 8 | 350.0 | 165.0 | 3693.0 | 11.5 | 70 | 1 | \"buick skylark 320\" |  \n",
    "| 18.0 | 8 | 318.0 | 150.0 | 3436.0 | 11.0 | 70 | 1 | \"plymouth satellite\" |\n",
    "| 26.0 | 4 | 97.00 | 46.00 | 1835.0 | 20.5 | 70 | 2 | \"volkswagen 1131 deluxe sedan\" |\n",
    "| 25.0 | 4 | 110.0 | 87.00 | 2672.0 | 17.5 | 70 | 2 | \"peugeot 504\" |\n",
    "| 24.0 | 4 | 107.0 | 90.00 | 2430.0 | 14.5 | 70 | 2 | \"audi 100 ls\" |\n",
    "| 25.0 | 4 | 113.0 | 95.00 | 2228.0 | 14.0 | 71 | 3 | \"toyota corona\" |\n",
    "| 27.0 | 4 | 97.00 | 88.00 | 2130.0 | 14.5 | 70 | 3 | \"datsun pl510\" |\n",
    "| 35.0 | 4 | 72.00 | 69.00 | 1613.0 | 18.0 | 71 | 3 | \"datsun 1200\" |\n",
    "\n",
    "Let's encode it properly using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's understand what the origin values represent\n",
    "print(\"Value counts for 'origin':\")\n",
    "print(df['origin'].value_counts())\n",
    "\n",
    "# Convert 'origin' to a more meaningful categorical representation\n",
    "origin_mapping = {1: 'america', 2: 'europe', 3: 'asia'}\n",
    "df['origin_name'] = df['origin'].map(origin_mapping)\n",
    "\n",
    "# Apply one-hot encoding to the 'origin' column\n",
    "origin_encoded = pd.get_dummies(df['origin_name'], prefix='origin')\n",
    "\n",
    "# Join the encoded columns to the original dataframe\n",
    "df = pd.concat([df, origin_encoded], axis=1)\n",
    "\n",
    "# Drop the original 'origin' and 'origin_name' columns\n",
    "df = df.drop(['origin', 'origin_name'], axis=1)\n",
    "\n",
    "# Display the first few rows to confirm the encoding\n",
    "print(\"\\nFirst 5 rows after encoding 'origin':\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f07f7b6",
   "metadata": {},
   "source": [
    "### 2.2 Explore data patterns and distributions\n",
    "Now let's visualize our dataset's key features to better understand their distributions and relationships, starting with histograms of our numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef8bc4",
   "metadata": {},
   "source": [
    "#### 2.2.1 Create histograms, boxplots, and count plots for categorical variables (as applicable).\n",
    "\n",
    "Let's create visualizations to understand the distributions of our features. These plots will help us identify patterns and relationships in the data that might influence our modeling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbdbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for numerical features\n",
    "plt.figure(figsize=(15, 10))\n",
    "numerical_features = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year']\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(df[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplots for numerical features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(y=df[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.ylabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b009707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count plots for categorical variables (the encoded origin columns)\n",
    "plt.figure(figsize=(10, 6))\n",
    "origin_counts = df[['origin_america', 'origin_asia', 'origin_europe']].sum()\n",
    "sns.barplot(x=origin_counts.index, y=origin_counts.values)\n",
    "plt.title('Distribution of Cars by Origin')\n",
    "plt.xlabel('Origin')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504cb0a",
   "metadata": {},
   "source": [
    "#### 2.2.2 Identify patterns, outliers, and anomalies in feature distributions.\n",
    "\n",
    "Now, let's examine relationships between features and identify potential outliers. Correlation analysis and scatter plots will help us understand how features relate to our target variable (mpg) and to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix to identify relationships between numerical features\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plots of important features vs. target variable (mpg)\n",
    "plt.figure(figsize=(15, 10))\n",
    "features_to_plot = ['displacement', 'horsepower', 'weight', 'acceleration']\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.scatterplot(x=df[feature], y=df['mpg'], hue=df['model_year'])\n",
    "    plt.title(f'{feature} vs. mpg')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('mpg')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pair plots for key features to identify patterns and outliers\n",
    "sns.pairplot(df[['mpg', 'displacement', 'horsepower', 'weight', 'model_year']])\n",
    "plt.suptitle('Pair Plots of Key Features', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and show potential outliers using z-score method\n",
    "from scipy import stats\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    z_scores = np.abs(stats.zscore(df[feature]))\n",
    "    outliers = (z_scores > 3)\n",
    "    \n",
    "    # Plot histograms with outliers highlighted\n",
    "    sns.histplot(df[feature], kde=True, color='blue', alpha=0.5)\n",
    "    if outliers.any():\n",
    "        sns.histplot(df[feature][outliers], color='red', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Distribution of {feature} with Outliers')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Print number of outliers\n",
    "    outlier_count = outliers.sum()\n",
    "    if outlier_count > 0:\n",
    "        print(f\"{feature} has {outlier_count} outliers (z-score > 3)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fb0ec",
   "metadata": {},
   "source": [
    "#### 2.2.3 Check for class imbalance in the target variable (as applicable).\n",
    "\n",
    "While our target variable (mpg) is continuous for regression, we'll examine its distribution to ensure it's well-represented across its range and doesn't have concentrated values that could bias our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the distribution of the target variable (mpg)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['mpg'], bins=20, kde=True)\n",
    "plt.title('Distribution of MPG (Target Variable)')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(df['mpg'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"mpg\"].mean():.2f}')\n",
    "plt.axvline(df['mpg'].median(), color='green', linestyle='--', label=f'Median: {df[\"mpg\"].median():.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create a violin plot of mpg by number of cylinders\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='cylinders', y='mpg', data=df)\n",
    "plt.title('Distribution of MPG by Number of Cylinders')\n",
    "plt.xlabel('Number of Cylinders')\n",
    "plt.ylabel('Miles Per Gallon (MPG)')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de323733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mpg distribution by origin\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.kdeplot(df[df['origin_america'] == True]['mpg'], fill=True, label='America')\n",
    "plt.title('MPG Distribution for American Cars')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.kdeplot(df[df['origin_europe'] == True]['mpg'], fill=True, label='Europe')\n",
    "plt.title('MPG Distribution for European Cars')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.kdeplot(df[df['origin_asia'] == True]['mpg'], fill=True, label='Asia')\n",
    "plt.title('MPG Distribution for Asian Cars')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace217a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also look at MPG over the years to detect any trends\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='model_year', y='mpg', data=df)\n",
    "plt.title('MPG Distribution by Model Year')\n",
    "plt.xlabel('Model Year')\n",
    "plt.ylabel('Miles Per Gallon (MPG)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d87d",
   "metadata": {},
   "source": [
    "### 2.3 Feature selection and engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1394f17",
   "metadata": {},
   "source": [
    "#### 2.3.1 Create new features (as applicable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: Extract car make from car_name\n",
    "\n",
    "# Extract the car make (first word) from the car_name column\n",
    "df['make'] = df['car_name'].str.split().str[0]\n",
    "\n",
    "# Display the first 10 rows with the new 'make' column\n",
    "print(\"First 10 rows with make column added:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b18d9f",
   "metadata": {},
   "source": [
    "#### 2.3.2 Transform or combine existing features to improve model performance (as applicable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fadc7a",
   "metadata": {},
   "source": [
    "##### Transform Makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb566a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping dictionary for make standardization\n",
    "make_mapping = {\n",
    "    'chevy': 'chevrolet',\n",
    "    'chevroelt': 'chevrolet',\n",
    "    'vokswagen': 'volkswagen',\n",
    "    'vw': 'volkswagen',\n",
    "    'toyouta': 'toyota',\n",
    "    'mercedes': 'mercedes-benz',\n",
    "    'maxda': 'mazda',\n",
    "    'hi': 'hindustan',    # 'hi' appears to be Hindustan Motors\n",
    "    'capri': 'ford'       # Capri was a model made by Ford\n",
    "}\n",
    "\n",
    "# Apply the mapping to standardize makes\n",
    "df['make'] = df['make'].replace(make_mapping)\n",
    "\n",
    "# Show the updated make counts\n",
    "make_counts = df['make'].value_counts()\n",
    "print(\"Updated make counts after standardization:\")\n",
    "print(make_counts)\n",
    "\n",
    "# Display the first 10 rows to verify changes\n",
    "print(\"\\nFirst 10 rows with standardized make column:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All the unique makes\")\n",
    "print(make_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b974317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to check if each make has exactly one origin\n",
    "def check_make_origin_consistency(df):\n",
    "    # Get unique makes\n",
    "    makes = df['make'].unique()\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    make_origins = {}\n",
    "    inconsistent_makes = {}\n",
    "    \n",
    "    # Check each make\n",
    "    for make in makes:\n",
    "        # Get data for this make\n",
    "        make_data = df[df['make'] == make]\n",
    "        \n",
    "        # Check which origins this make has\n",
    "        has_america = make_data['origin_america'].any()\n",
    "        has_asia = make_data['origin_asia'].any()\n",
    "        has_europe = make_data['origin_europe'].any()\n",
    "        \n",
    "        # Count number of origins\n",
    "        origin_count = sum([has_america, has_asia, has_europe])\n",
    "        \n",
    "        # Store the origin\n",
    "        if has_america:\n",
    "            make_origins[make] = 'America'\n",
    "        elif has_asia:\n",
    "            make_origins[make] = 'Asia'\n",
    "        elif has_europe:\n",
    "            make_origins[make] = 'Europe'\n",
    "        \n",
    "        # If more than one origin, this make is inconsistent\n",
    "        if origin_count > 1:\n",
    "            inconsistent_makes[make] = {\n",
    "                'America': has_america,\n",
    "                'Asia': has_asia,\n",
    "                'Europe': has_europe\n",
    "            }\n",
    "    \n",
    "    return make_origins, inconsistent_makes\n",
    "\n",
    "# Run the check\n",
    "make_origins, inconsistent_makes = check_make_origin_consistency(df)\n",
    "\n",
    "# Show results\n",
    "print(\"Number of unique makes:\", len(make_origins))\n",
    "print(\"\\nOrigin countries by make:\")\n",
    "for make, origin in sorted(make_origins.items()):\n",
    "    print(f\"{make}: {origin}\")\n",
    "\n",
    "if inconsistent_makes:\n",
    "    print(\"\\nWARNING: The following makes have inconsistent origins:\")\n",
    "    for make, origins in inconsistent_makes.items():\n",
    "        print(f\"{make}: {origins}\")\n",
    "else:\n",
    "    print(\"\\nAll makes have consistent origins!\")\n",
    "\n",
    "# Create a correlation matrix visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Create a crosstab between make and origin\n",
    "make_origin_crosstab = pd.crosstab(df['make'], [df['origin_america'], df['origin_asia'], df['origin_europe']])\n",
    "print(\"\\nCrosstab of make and origin:\")\n",
    "display(make_origin_crosstab)\n",
    "\n",
    "# Create a heatmap showing make-origin relationships\n",
    "plt.figure(figsize=(15, 12))\n",
    "# Define a palette that has True as red and False as white/light\n",
    "# First, create a temporary DataFrame with just the origins\n",
    "origin_df = df.groupby('make')[['origin_america', 'origin_asia', 'origin_europe']].first()\n",
    "sns.heatmap(origin_df, cmap=['white', 'blue'], cbar=False)\n",
    "plt.title('Origin by Make (Blue indicates True)')\n",
    "plt.ylabel('Make')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value counts of each make\n",
    "make_counts = df['make'].value_counts()\n",
    "\n",
    "\n",
    "# Identify makes with fewer than 10 instances\n",
    "rare_makes = make_counts[make_counts < 10]\n",
    "print(\"Makes with fewer than 5 instances:\")\n",
    "print(rare_makes)\n",
    "\n",
    "# Calculate the total number of cars with these rare makes\n",
    "rare_makes_count = sum(rare_makes)\n",
    "total_cars = len(df)\n",
    "rare_percentage = (rare_makes_count / total_cars) * 100\n",
    "\n",
    "print(f\"\\nTotal cars in dataset: {total_cars}\")\n",
    "print(f\"Cars with rare makes (< 5 instances): {rare_makes_count}\")\n",
    "print(f\"Percentage of dataset with rare makes: {rare_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the original dataframe size\n",
    "original_size = len(df)\n",
    "\n",
    "# Get the list of rare makes (makes with fewer than 5 instances)\n",
    "rare_makes_list = rare_makes.index.tolist()\n",
    "\n",
    "# Filter out rows with rare makes\n",
    "df_filtered = df[~df['make'].isin(rare_makes_list)]\n",
    "\n",
    "# Calculate the new size\n",
    "new_size = len(df_filtered)\n",
    "removed_rows = original_size - new_size\n",
    "\n",
    "# Display results\n",
    "print(f\"Original dataset size: {original_size} rows\")\n",
    "print(f\"After removing rare makes: {new_size} rows\")\n",
    "print(f\"Removed {removed_rows} rows ({(removed_rows/original_size)*100:.2f}% of data)\")\n",
    "\n",
    "# Check the distribution of makes in the filtered dataset\n",
    "print(\"\\nMake distribution in filtered dataset:\")\n",
    "make_distribution = df_filtered['make'].value_counts()\n",
    "print(make_distribution)\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "print(\"\\nFirst 5 rows of filtered dataset:\")\n",
    "display(df_filtered.head())\n",
    "\n",
    "# Optionally, reassign to the original variable if you want to continue with the filtered dataset\n",
    "# df = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most common makes (to avoid too many colors in the plot)\n",
    "top_makes = df['make'].value_counts().head(10).index.tolist()\n",
    "print(f\"Top 10 makes: {top_makes}\")\n",
    "\n",
    "# Filter dataset to include only the top makes for better visualization\n",
    "filtered_df = df[df['make'].isin(top_makes)]\n",
    "\n",
    "# Create a figure with adequate size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Use a color palette that distinguishes between different makes\n",
    "colors = sns.color_palette(\"husl\", len(top_makes))\n",
    "\n",
    "# Plot KDE curves for each make\n",
    "for i, make in enumerate(top_makes):\n",
    "    make_data = filtered_df[filtered_df['make'] == make]\n",
    "    sns.kdeplot(make_data['mpg'], fill=True, color=colors[i], alpha=0.4, label=make, common_norm=False)\n",
    "\n",
    "# Add plot details\n",
    "plt.title('MPG Distribution by Car Make', fontsize=16)\n",
    "plt.xlabel('Miles Per Gallon (MPG)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add a legend at the bottom\n",
    "plt.legend(title='Car Make', bbox_to_anchor=(0.5, -0.15), loc='upper center', ncol=5)\n",
    "\n",
    "# Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Optionally, add some statistics for each make\n",
    "print(\"\\nMPG Statistics by Make:\")\n",
    "for make in top_makes:\n",
    "    make_data = filtered_df[filtered_df['make'] == make]\n",
    "    print(f\"{make.capitalize()}:\")\n",
    "    print(f\"  Count: {len(make_data)}\")\n",
    "    print(f\"  Average MPG: {make_data['mpg'].mean():.2f}\")\n",
    "    print(f\"  Min-Max: {make_data['mpg'].min():.1f}-{make_data['mpg'].max():.1f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806cbdcb",
   "metadata": {},
   "source": [
    "#### 2.3.3 Scale or normalize data (as applicable).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdd9f3",
   "metadata": {},
   "source": [
    "### Reflection 2: What patterns or anomalies do you see? Do any features stand out? What preprocessing steps were necessary to clean and improve the data? Did you create or modify any features to improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a1cd5",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ecd67",
   "metadata": {},
   "source": [
    "### 3.1 Choose features and target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5d4a2",
   "metadata": {},
   "source": [
    "Model Year\n",
    "As a numerical feature:\n",
    "\n",
    "Preserves the chronological relationship between years\n",
    "Implicitly models the steady improvement in fuel efficiency over time\n",
    "Simpler model with fewer parameters\n",
    "\n",
    "\n",
    "As a categorical feature:\n",
    "\n",
    "Allows for non-linear relationships between specific years and mpg\n",
    "May capture regulatory changes that happened in specific years\n",
    "Increases model complexity significantly\n",
    "\n",
    "\n",
    "\n",
    "Since the data shows a fairly steady upward trend in mpg by model year (as seen in your boxplots), treating it as a numerical feature is likely sufficient and won't cause overfitting. The linear relationship appears reasonably strong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4113d",
   "metadata": {},
   "source": [
    "#### 3.1.1 Select two or more input features\n",
    "\n",
    "We are going to use model year, weight, and make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575125c",
   "metadata": {},
   "source": [
    "#### 3.1.2 Select a target variable (as applicable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9ded4",
   "metadata": {},
   "source": [
    "The assignment indicated we ought to use MPG. I was hoping I could feed data and use it to predict the origin of the car or the make, and I might in the future as time allows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25790a",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1.3 Justify your selection with reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0feebf",
   "metadata": {},
   "source": [
    "### 3.2 Define X and y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c910b32",
   "metadata": {},
   "source": [
    "#### 3.2.1 Assign input features to X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452585e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2.1 Assign input features to X\n",
    "\n",
    "# Select the features we want to use for our model\n",
    "# Convert 'make' to a categorical variable using one-hot encoding\n",
    "make_encoded = pd.get_dummies(df['make'], prefix='make', drop_first=True)\n",
    "\n",
    "# Combine the encoded make columns with our other numerical features\n",
    "X = pd.concat([\n",
    "    df[['weight', 'model_year']], \n",
    "    make_encoded\n",
    "], axis=1)\n",
    "\n",
    "# Display the first few rows of our feature set\n",
    "print(\"First 5 rows of our features (X):\")\n",
    "display(X.head())\n",
    "\n",
    "# Shape of the feature matrix\n",
    "print(f\"\\nShape of X: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4506996",
   "metadata": {},
   "source": [
    "#### 3.2.2 Assign target variable to y (as applicable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2.2 Assign target variable to y\n",
    "\n",
    "# Our target variable is 'mpg'\n",
    "y = df['mpg']\n",
    "\n",
    "# Display the first few values of our target\n",
    "print(\"First 5 values of our target (y):\")\n",
    "display(y.head())\n",
    "\n",
    "# Basic statistics of our target variable\n",
    "print(\"\\nBasic statistics of MPG (target variable):\")\n",
    "print(y.describe())\n",
    "\n",
    "# Visualize the distribution of our target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y, kde=True)\n",
    "plt.title('Distribution of MPG (Target Variable)')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52638da",
   "metadata": {},
   "source": [
    "### Reflection 3: Why did you choose these features? How might they impact predictions or accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8df40",
   "metadata": {},
   "source": [
    "## Section 4. Train a Model (Linear Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c893a",
   "metadata": {},
   "source": [
    "### 4.1 Split the data into training and test sets using train_test_split (or StratifiedShuffleSplit if class imbalance is an issue).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb31b4e",
   "metadata": {},
   "source": [
    "#### 4.1.1 Initial Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f40604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Split the data into training and test sets\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(state_setter)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=state_setter)\n",
    "\n",
    "# Display the shapes of our training and testing sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Create a better comparison of MPG distributions with same bins and normalization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Define common binning\n",
    "bins = np.linspace(5, 50, 15)  # 15 bins from 5 to 50 MPG\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(y_train, bins=bins, kde=True, color='blue', stat='density')\n",
    "plt.title('MPG Distribution - Training Set')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_test, bins=bins, kde=True, color='green', stat='density')\n",
    "plt.title('MPG Distribution - Testing Set')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare statistical measures\n",
    "print(\"\\nTraining set MPG statistics:\")\n",
    "print(y_train.describe())\n",
    "print(\"\\nTesting set MPG statistics:\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135cd982",
   "metadata": {},
   "source": [
    "These distributions are concerningly different. Let's tackle it with a stratified test-train-split...but stratify it across what? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e48c5",
   "metadata": {},
   "source": [
    "#### 4.1.2 Stratified Test Train Split - Stratification Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef2d54",
   "metadata": {},
   "source": [
    "Per our research earlier, there are too many small 'make' bins, so let's consider origin instead. Let's also look at separating MPG into Bins and stratifying according to the bins. We will them compare mpg distributions produced by these 2 methods against the random split we've done here and the original data set's mpg distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to evaluate similarity between distributions\n",
    "def compare_distributions(original, sample, method_name):\n",
    "    # Calculate statistical distances\n",
    "    ks_stat, ks_pval = ks_2samp(original, sample)\n",
    "    w_distance = wasserstein_distance(original, sample)\n",
    "    e_distance = energy_distance(original, sample)\n",
    "    \n",
    "    # Calculate basic statistics and their differences\n",
    "    orig_stats = original.describe()\n",
    "    sample_stats = sample.describe()\n",
    "    \n",
    "    # Calculate absolute differences in key statistics\n",
    "    mean_diff = abs(orig_stats['mean'] - sample_stats['mean'])\n",
    "    std_diff = abs(orig_stats['std'] - sample_stats['std'])\n",
    "    \n",
    "    # Quartile differences\n",
    "    q1_diff = abs(orig_stats['25%'] - sample_stats['25%'])\n",
    "    median_diff = abs(orig_stats['50%'] - sample_stats['50%'])\n",
    "    q3_diff = abs(orig_stats['75%'] - sample_stats['75%'])\n",
    "    \n",
    "   \n",
    "    # Return a composite score \n",
    "    # Weighted sum of distances\n",
    "    composite_score = (ks_stat * 0.3) + (w_distance * 0.3) + (e_distance * 0.1) + \\\n",
    "                      (mean_diff * 0.1) + (std_diff * 0.1) + \\\n",
    "                      (q1_diff * 0.03) + (median_diff * 0.04) + (q3_diff * 0.03)\n",
    "    \n",
    "    \"\"\"The score is a weighted combination of several statistical distance metrics:\n",
    "\n",
    "    Kolmogorov-Smirnov statistic (30% weight): Measures the maximum difference between two cumulative distribution functions. A value of 0 means identical distributions.\n",
    "    Wasserstein distance (30% weight): Also known as the \"Earth Mover's Distance,\" it measures how much \"work\" it would take to transform one distribution into another. Lower values mean distributions are more similar.\n",
    "    Energy distance (10% weight): Another statistical distance metric that's sensitive to differences in both shape and location of distributions.\n",
    "    Differences in key statistics (30% total weight):\n",
    "\n",
    "    Mean difference (10%)\n",
    "    Standard deviation difference (10%)\n",
    "    Quartile differences (10% total: 3% for Q1, 4% for median, 3% for Q3)\n",
    "\n",
    "    These components capture different aspects of distribution similarity:\n",
    "\n",
    "    The statistical distance metrics (KS, Wasserstein, Energy) capture overall shape differences\n",
    "    The mean and standard deviation differences capture central tendency and spread\n",
    "    The quartile differences capture structural details of the distribution\"\"\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'ks_stat': ks_stat,\n",
    "        'ks_pval': ks_pval,\n",
    "        'w_distance': w_distance,\n",
    "        'e_distance': e_distance,\n",
    "        'mean_diff': mean_diff,\n",
    "        'std_diff': std_diff,\n",
    "        'q1_diff': q1_diff,\n",
    "        'median_diff': median_diff,\n",
    "        'q3_diff': q3_diff,\n",
    "        'composite_score': composite_score\n",
    "    }\n",
    "\n",
    "# Original data\n",
    "y_original = df['mpg']\n",
    "\n",
    "# Prepare features (same as before)\n",
    "make_encoded = pd.get_dummies(df['make'], prefix='make', drop_first=True)\n",
    "X = pd.concat([df[['weight', 'model_year']], make_encoded], axis=1)\n",
    "\n",
    "# Method 1: Stratify by origin\n",
    "strat_var_origin = np.where(df['origin_america'], 'america', \n",
    "                   np.where(df['origin_europe'], 'europe', 'asia'))\n",
    "\n",
    "X_train_origin, X_test_origin, y_train_origin, y_test_origin = train_test_split(\n",
    "    X, y_original, \n",
    "    test_size=0.2, \n",
    "    random_state=state_setter,\n",
    "    stratify=strat_var_origin\n",
    ")\n",
    "\n",
    "# Method 2: Stratify by 5 binned MPG values\n",
    "mpg_bins = pd.qcut(df['mpg'], q=5, labels=False, duplicates='drop')\n",
    "\n",
    "X_train_mpg, X_test_mpg, y_train_mpg, y_test_mpg = train_test_split(\n",
    "    X, y_original, \n",
    "    test_size=0.2, \n",
    "    random_state=state_setter,\n",
    "    stratify=mpg_bins\n",
    ")\n",
    "\n",
    "# Method 3: Regular random split (as baseline)\n",
    "X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(\n",
    "    X, y_original, \n",
    "    test_size=0.2, \n",
    "    random_state=state_setter\n",
    ")\n",
    "\n",
    "# Compare distributions\n",
    "origin_results = compare_distributions(y_original, y_test_origin, \"Origin\")\n",
    "mpg_results = compare_distributions(y_original, y_test_mpg, \"MPG Bins\")\n",
    "random_results = compare_distributions(y_original, y_test_rand, \"Random\")\n",
    "# Visualize all distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Define common binning\n",
    "bins = np.linspace(5, 50, 15)\n",
    "\n",
    "# Original distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(y_original, bins=bins, kde=True, color='purple', stat='density')\n",
    "plt.title('Original MPG Distribution')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "# Origin stratification\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(y_test_origin, bins=bins, kde=True, color='blue', stat='density')\n",
    "plt.title(f'Test Set - Stratified by Origin\\nScore: {origin_results[\"composite_score\"]:.4f}')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "# MPG bins stratification\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(y_test_mpg, bins=bins, kde=True, color='green', stat='density')\n",
    "plt.title(f'Test Set - Stratified by MPG Bins\\nScore: {mpg_results[\"composite_score\"]:.4f}')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "# Random split\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(y_test_rand, bins=bins, kde=True, color='red', stat='density')\n",
    "plt.title(f'Test Set - Random Split\\nScore: {random_results[\"composite_score\"]:.4f}')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparison of Distribution Preservation Methods (Lower Score = Better Match)', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ef6a2",
   "metadata": {},
   "source": [
    "This suggests mpg bins is the best, and visually it definitely appears closest. Given the small set of the data however, let's cross-validate across 10 splits to confirm we achieve the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e931e3",
   "metadata": {},
   "source": [
    "Note - the score represents the dissimilarity between the test set distribution and the original distribution. Lower scores indicate test sets that more closely match the original MPG distribution, which is why lower is better, and 0 would mean the distributions are identical.\n",
    "\n",
    "If curious for more detail, review the comments and calculations in the python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1658154",
   "metadata": {},
   "source": [
    "#### 4.1.3 Cross Validated Stratification Factor Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate similarity between distributions\n",
    "def compare_distributions(original, sample, method_name):\n",
    "    # Calculate statistical distances\n",
    "    ks_stat, ks_pval = ks_2samp(original, sample)\n",
    "    w_distance = wasserstein_distance(original, sample)\n",
    "    e_distance = energy_distance(original, sample)\n",
    "    \n",
    "    # Calculate basic statistics and their differences\n",
    "    orig_stats = original.describe()\n",
    "    sample_stats = sample.describe()\n",
    "    \n",
    "    # Calculate absolute differences in key statistics\n",
    "    mean_diff = abs(orig_stats['mean'] - sample_stats['mean'])\n",
    "    std_diff = abs(orig_stats['std'] - sample_stats['std'])\n",
    "    \n",
    "    # Quartile differences\n",
    "    q1_diff = abs(orig_stats['25%'] - sample_stats['25%'])\n",
    "    median_diff = abs(orig_stats['50%'] - sample_stats['50%'])\n",
    "    q3_diff = abs(orig_stats['75%'] - sample_stats['75%'])\n",
    "    \n",
    "    # Return a composite score  \n",
    "    # Weighted sum of distances\n",
    "    composite_score = (ks_stat * 0.3) + (w_distance * 0.3) + (e_distance * 0.1) + \\\n",
    "                      (mean_diff * 0.1) + (std_diff * 0.1) + \\\n",
    "                      (q1_diff * 0.03) + (median_diff * 0.04) + (q3_diff * 0.03)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'ks_stat': ks_stat,\n",
    "        'ks_pval': ks_pval,\n",
    "        'w_distance': w_distance,\n",
    "        'e_distance': e_distance,\n",
    "        'mean_diff': mean_diff,\n",
    "        'std_diff': std_diff,\n",
    "        'q1_diff': q1_diff,\n",
    "        'median_diff': median_diff,\n",
    "        'q3_diff': q3_diff,\n",
    "        'composite_score': composite_score\n",
    "    }\n",
    "\n",
    "# Original data\n",
    "y_original = df['mpg']\n",
    "\n",
    "# Prepare features (same as before)\n",
    "make_encoded = pd.get_dummies(df['make'], prefix='make', drop_first=True)\n",
    "X = pd.concat([df[['weight', 'model_year']], make_encoded], axis=1)\n",
    "\n",
    "# Number of cross-validation iterations\n",
    "n_iterations = 10\n",
    "\n",
    "# Initialize dictionaries to store results and test set distributions\n",
    "results_origin = []\n",
    "results_mpg = []\n",
    "results_random = []\n",
    "\n",
    "# Initialize arrays to accumulate histograms for averaging\n",
    "all_y_test_origin = []\n",
    "all_y_test_mpg = []\n",
    "all_y_test_random = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for i in range(n_iterations):\n",
    "    # Set random state based on state_setter + iteration\n",
    "    random_state = state_setter + i\n",
    "    \n",
    "    # Method 1: Stratify by origin\n",
    "    strat_var_origin = np.where(df['origin_america'], 'america', \n",
    "                    np.where(df['origin_europe'], 'europe', 'asia'))\n",
    "\n",
    "    X_train_origin, X_test_origin, y_train_origin, y_test_origin = train_test_split(\n",
    "        X, y_original, \n",
    "        test_size=0.2, \n",
    "        random_state=random_state,\n",
    "        stratify=strat_var_origin\n",
    "    )\n",
    "    \n",
    " \n",
    "    mpg_bins = pd.qcut(df['mpg'], q=5, labels=False, duplicates='drop')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X_train_mpg, X_test_mpg, y_train_mpg, y_test_mpg = train_test_split(\n",
    "        X, y_original, \n",
    "        test_size=0.2, \n",
    "        random_state=random_state,\n",
    "        stratify=mpg_bins\n",
    "    )\n",
    "    \n",
    "    # Method 3: Regular random split (as baseline)\n",
    "    X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(\n",
    "        X, y_original, \n",
    "        test_size=0.2, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Evaluate distributions\n",
    "    results_origin.append(compare_distributions(y_original, y_test_origin, \"Origin\"))\n",
    "    results_mpg.append(compare_distributions(y_original, y_test_mpg, \"MPG Bins\"))\n",
    "    results_random.append(compare_distributions(y_original, y_test_rand, \"Random\"))\n",
    "    \n",
    "    # Store test set distributions for later averaging\n",
    "    all_y_test_origin.append(y_test_origin)\n",
    "    all_y_test_mpg.append(y_test_mpg)\n",
    "    all_y_test_random.append(y_test_rand)\n",
    "\n",
    "# Calculate average scores for each method\n",
    "def average_results(results_list):\n",
    "    avg_results = {\n",
    "        'ks_stat': np.mean([r['ks_stat'] for r in results_list]),\n",
    "        'ks_pval': np.mean([r['ks_pval'] for r in results_list]),\n",
    "        'w_distance': np.mean([r['w_distance'] for r in results_list]),\n",
    "        'e_distance': np.mean([r['e_distance'] for r in results_list]),\n",
    "        'mean_diff': np.mean([r['mean_diff'] for r in results_list]),\n",
    "        'std_diff': np.mean([r['std_diff'] for r in results_list]),\n",
    "        'q1_diff': np.mean([r['q1_diff'] for r in results_list]),\n",
    "        'median_diff': np.mean([r['median_diff'] for r in results_list]),\n",
    "        'q3_diff': np.mean([r['q3_diff'] for r in results_list]),\n",
    "        'composite_score': np.mean([r['composite_score'] for r in results_list])\n",
    "    }\n",
    "    return avg_results\n",
    "\n",
    "avg_origin = average_results(results_origin)\n",
    "avg_mpg = average_results(results_mpg)\n",
    "avg_random = average_results(results_random)\n",
    "\n",
    "# Function to create a combined histogram representation across all CV iterations\n",
    "def create_averaged_histogram(all_samples, bins=15):\n",
    "    # Concatenate all samples across CV iterations\n",
    "    combined = pd.concat(all_samples)\n",
    "    return combined\n",
    "\n",
    "# Create averaged histograms\n",
    "avg_hist_origin = create_averaged_histogram(all_y_test_origin)\n",
    "avg_hist_mpg = create_averaged_histogram(all_y_test_mpg)\n",
    "avg_hist_random = create_averaged_histogram(all_y_test_random)\n",
    "\n",
    "# Visualize the average distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Define common binning\n",
    "bins = np.linspace(5, 50, 15)\n",
    "\n",
    "# Original distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(y_original, bins=bins, kde=True, color='purple', stat='density')\n",
    "plt.title('Original MPG Distribution')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "# Origin stratification (average across CV)\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(avg_hist_origin, bins=bins, kde=True, color='blue', stat='density')\n",
    "plt.title(f'Stratified by Origin\\nAvg Score: {avg_origin[\"composite_score\"]:.4f}')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "# MPG bins stratification (average across CV)\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(avg_hist_mpg, bins=bins, kde=True, color='green', stat='density')\n",
    "plt.title(f'Test Set - Stratified by MPG Bins\\nAvg Score: {avg_mpg[\"composite_score\"]:.4f}')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "# Random split (average across CV)\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(avg_hist_random, bins=bins, kde=True, color='red', stat='density')\n",
    "plt.title(f'Test Set - Random Split\\nAvg Score: {avg_random[\"composite_score\"]:.4f}')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparison of Distribution Preservation Methods (10-fold CV)\\n(Lower Score = Better Match)', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Create a summary table of average results\n",
    "methods = [\n",
    "    {'method': 'Origin', **avg_origin},\n",
    "    {'method': 'MPG Bins', **avg_mpg},\n",
    "    {'method': 'Random', **avg_random}\n",
    "]\n",
    "comparison_df = pd.DataFrame(methods)\n",
    "comparison_df = comparison_df.set_index('method')\n",
    "\n",
    "# Sort by composite score (lower is better)\n",
    "comparison_df = comparison_df.sort_values('composite_score')\n",
    "\n",
    "print(\"\\n--- Summary of Average Distribution Similarity Across 10 Splits ---\")\n",
    "print(comparison_df[['ks_stat', 'w_distance', 'mean_diff', 'std_diff', 'composite_score']])\n",
    "\n",
    "# Determine the winner\n",
    "winner = comparison_df.index[0]\n",
    "print(f\"\\nBest stratification method: {winner}\")\n",
    "\n",
    "# Show standard deviation of scores to see consistency\n",
    "std_origin = np.std([r['composite_score'] for r in results_origin])\n",
    "std_mpg = np.std([r['composite_score'] for r in results_mpg])\n",
    "std_random = np.std([r['composite_score'] for r in results_random])\n",
    "\n",
    "print(\"\\n--- Standard Deviation of Scores Across 10 Splits ---\")\n",
    "print(f\"Origin: {std_origin:.4f}\")\n",
    "print(f\"MPG Bins: {std_mpg:.4f}\")\n",
    "print(f\"Random: {std_random:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e601c5d",
   "metadata": {},
   "source": [
    "This clearly indicates the mpg binning method gave the lowest score, and a distribution closest to the original. No surprise given the distribution of mpg itself is what we're interested in! \n",
    "\n",
    "Stratifying the split across a greater number of bins should obviously help it match the the original distribution, but let's double check that and also confirm there aren't major performance implications. With our smaller data set, it probably won't have performance implications, but let's double check and confirm a larger number of bins is ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ec2b7",
   "metadata": {},
   "source": [
    "#### 4.1.4 Bin Count Stratification Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate similarity between distributions (same as before)\n",
    "def compare_distributions(original, sample, method_name):\n",
    "    # Calculate statistical distances\n",
    "    ks_stat, ks_pval = ks_2samp(original, sample)\n",
    "    w_distance = wasserstein_distance(original, sample)\n",
    "    e_distance = energy_distance(original, sample)\n",
    "    \n",
    "    # Calculate basic statistics and their differences\n",
    "    orig_stats = original.describe()\n",
    "    sample_stats = sample.describe()\n",
    "    \n",
    "    # Calculate absolute differences in key statistics\n",
    "    mean_diff = abs(orig_stats['mean'] - sample_stats['mean'])\n",
    "    std_diff = abs(orig_stats['std'] - sample_stats['std'])\n",
    "    \n",
    "    # Quartile differences\n",
    "    q1_diff = abs(orig_stats['25%'] - sample_stats['25%'])\n",
    "    median_diff = abs(orig_stats['50%'] - sample_stats['50%'])\n",
    "    q3_diff = abs(orig_stats['75%'] - sample_stats['75%'])\n",
    "    \n",
    "    # Return a composite score (lower is better)\n",
    "    composite_score = (ks_stat * 0.3) + (w_distance * 0.3) + (e_distance * 0.1) + \\\n",
    "                      (mean_diff * 0.1) + (std_diff * 0.1) + \\\n",
    "                      (q1_diff * 0.03) + (median_diff * 0.04) + (q3_diff * 0.03)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'ks_stat': ks_stat,\n",
    "        'ks_pval': ks_pval,\n",
    "        'w_distance': w_distance,\n",
    "        'e_distance': e_distance,\n",
    "        'mean_diff': mean_diff,\n",
    "        'std_diff': std_diff,\n",
    "        'q1_diff': q1_diff,\n",
    "        'median_diff': median_diff,\n",
    "        'q3_diff': q3_diff,\n",
    "        'composite_score': composite_score\n",
    "    }\n",
    "\n",
    "# Original data\n",
    "y_original = df['mpg']\n",
    "\n",
    "# Prepare features\n",
    "make_encoded = pd.get_dummies(df['make'], prefix='make', drop_first=True)\n",
    "X = pd.concat([df[['weight', 'model_year']], make_encoded], axis=1)\n",
    "\n",
    "# Number of cross-validation iterations\n",
    "n_iterations = 10\n",
    "\n",
    "# Different bin counts to test\n",
    "bin_counts = [3, 5, 8, 15, 25]\n",
    "\n",
    "# Dictionary to store results for different bin counts\n",
    "bin_results = {}\n",
    "bin_times = {}\n",
    "all_distributions = {}\n",
    "\n",
    "# Baseline: Origin stratification for comparison\n",
    "all_y_test_origin = []\n",
    "results_origin = []\n",
    "origin_times = []\n",
    "\n",
    "# Run cross-validation for origin stratification as baseline\n",
    "for i in range(n_iterations):\n",
    "    random_state = state_setter + i\n",
    "    \n",
    "    # Measure time for origin stratification\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Method: Stratify by origin\n",
    "    strat_var_origin = np.where(df['origin_america'], 'america', \n",
    "                     np.where(df['origin_europe'], 'europe', 'asia'))\n",
    "\n",
    "    X_train_origin, X_test_origin, y_train_origin, y_test_origin = train_test_split(\n",
    "        X, y_original, \n",
    "        test_size=0.2, \n",
    "        random_state=random_state,\n",
    "        stratify=strat_var_origin\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    origin_times.append(end_time - start_time)\n",
    "    \n",
    "    # Evaluate distribution\n",
    "    results_origin.append(compare_distributions(y_original, y_test_origin, \"Origin\"))\n",
    "    all_y_test_origin.append(y_test_origin)\n",
    "\n",
    "# Average origin results\n",
    "avg_origin = {\n",
    "    'ks_stat': np.mean([r['ks_stat'] for r in results_origin]),\n",
    "    'w_distance': np.mean([r['w_distance'] for r in results_origin]),\n",
    "    'mean_diff': np.mean([r['mean_diff'] for r in results_origin]),\n",
    "    'std_diff': np.mean([r['std_diff'] for r in results_origin]),\n",
    "    'composite_score': np.mean([r['composite_score'] for r in results_origin])\n",
    "}\n",
    "avg_origin_time = np.mean(origin_times)\n",
    "\n",
    "# Run cross-validation for each bin count\n",
    "for bin_count in bin_counts:\n",
    "    method_name = f\"MPG-{bin_count}bins\"\n",
    "    bin_results[method_name] = []\n",
    "    bin_times[method_name] = []\n",
    "    all_distributions[method_name] = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        random_state = state_setter + i\n",
    "        \n",
    "        # Measure time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create bins and stratify\n",
    "        mpg_bins = pd.qcut(df['mpg'], q=bin_count, labels=False, duplicates='drop')\n",
    "\n",
    "        \n",
    "        X_train_bins, X_test_bins, y_train_bins, y_test_bins = train_test_split(\n",
    "            X, y_original, \n",
    "            test_size=0.2, \n",
    "            random_state=random_state,\n",
    "            stratify=mpg_bins\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        bin_times[method_name].append(end_time - start_time)\n",
    "        \n",
    "        # Evaluate distribution\n",
    "        bin_results[method_name].append(compare_distributions(y_original, y_test_bins, method_name))\n",
    "        all_distributions[method_name].append(y_test_bins)\n",
    "\n",
    "# Calculate averages for each bin count\n",
    "avg_results = {}\n",
    "for method_name in bin_results:\n",
    "    avg_results[method_name] = {\n",
    "        'ks_stat': np.mean([r['ks_stat'] for r in bin_results[method_name]]),\n",
    "        'w_distance': np.mean([r['w_distance'] for r in bin_results[method_name]]),\n",
    "        'mean_diff': np.mean([r['mean_diff'] for r in bin_results[method_name]]),\n",
    "        'std_diff': np.mean([r['std_diff'] for r in bin_results[method_name]]),\n",
    "        'composite_score': np.mean([r['composite_score'] for r in bin_results[method_name]])\n",
    "    }\n",
    "\n",
    "# Average times\n",
    "avg_times = {method: np.mean(times) for method, times in bin_times.items()}\n",
    "\n",
    "# Function to create a combined histogram\n",
    "def create_averaged_histogram(all_samples):\n",
    "    return pd.concat(all_samples)\n",
    "\n",
    "# Create averaged histograms\n",
    "avg_hist_origin = create_averaged_histogram(all_y_test_origin)\n",
    "avg_hist_bins = {method: create_averaged_histogram(dists) \n",
    "                for method, dists in all_distributions.items()}\n",
    "\n",
    "# Visualize the distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "bins = np.linspace(5, 50, 15)\n",
    "\n",
    "# Visualize the distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "bins = np.linspace(5, 50, 15)\n",
    "\n",
    "# Original distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.histplot(y_original, bins=bins, kde=True, color='purple', stat='density')\n",
    "plt.title('Original MPG Distribution')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "# Different bin counts\n",
    "colors = ['green', 'orange', 'red', 'brown', 'blue']  # Added blue as the 5th color\n",
    "subplot_positions = [2, 3, 4, 5, 6]  # Using all positions in the 2x3 grid\n",
    "\n",
    "for i, ((method, dist), color, pos) in enumerate(zip(avg_hist_bins.items(), colors, subplot_positions)):\n",
    "    plt.subplot(2, 3, pos)\n",
    "    sns.histplot(dist, bins=bins, kde=True, color=color, stat='density')\n",
    "    plt.title(f'{method}\\nScore: {avg_results[method][\"composite_score\"]:.4f}\\nTime: {avg_times[method]*1000:.2f} ms')\n",
    "    plt.xlabel('Miles Per Gallon (MPG)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(5, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('MPG Distribution Comparison with Different Bin Counts (10-fold CV)\\n(Lower Score = Better Match)', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Create a summary table\n",
    "summary_data = []\n",
    "\n",
    "# Add origin as baseline\n",
    "summary_data.append({\n",
    "    'Method': 'Origin',\n",
    "    'Score': avg_origin['composite_score'],\n",
    "    'Time (ms)': avg_origin_time * 1000,\n",
    "    'KS Stat': avg_origin['ks_stat'],\n",
    "    'W-Distance': avg_origin['w_distance'],\n",
    "    'Mean Diff': avg_origin['mean_diff'],\n",
    "    'Std Diff': avg_origin['std_diff']\n",
    "})\n",
    "\n",
    "# Add bin methods\n",
    "for method in avg_results:\n",
    "    summary_data.append({\n",
    "        'Method': method,\n",
    "        'Score': avg_results[method]['composite_score'],\n",
    "        'Time (ms)': avg_times[method] * 1000,\n",
    "        'KS Stat': avg_results[method]['ks_stat'],\n",
    "        'W-Distance': avg_results[method]['w_distance'],\n",
    "        'Mean Diff': avg_results[method]['mean_diff'],\n",
    "        'Std Diff': avg_results[method]['std_diff']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2911f",
   "metadata": {},
   "source": [
    "For our small data set, using a big bin count still worked out! We'll use 25 bins in our final stratification.\n",
    "\n",
    "A couple notes:\n",
    "- The 15 bin and 25 bin distributions were mapped to 12 bin diagrams for easy comparison across splits.\n",
    "- Bin Counts over 25, some bins only had a single sample and thus couldn't be stratified. 25 turned out to be the max we could support, which conveniently does not have performance implications for our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ae6f6",
   "metadata": {},
   "source": [
    "#### 4.1.5 Stratified Test Train Split\n",
    "We are finally ready to split the data! We will split it across 25 bins for mpg to keep the distribution equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.5 Test Train Split using MPG stratification with 25 bins\n",
    "\n",
    "# Original data\n",
    "y = df['mpg']\n",
    "\n",
    "# Prepare features\n",
    "make_encoded = pd.get_dummies(df['make'], prefix='make', drop_first=True)\n",
    "X = pd.concat([df[['weight', 'model_year']], make_encoded], axis=1)\n",
    "\n",
    "# Create 25 mpg bins for stratification (our optimal bin count from earlier analysis)\n",
    "mpg_bins = pd.qcut(df['mpg'], q=25, labels=False, duplicates='drop')\n",
    "\n",
    "# Split the data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=state_setter,\n",
    "    stratify=mpg_bins\n",
    ")\n",
    "\n",
    "# Display the shapes of our training and testing sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Visualize our final train/test distributions with the same number of bins\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Define a fixed set of bins for both plots - using exactly 12 bins\n",
    "fixed_bins = np.linspace(y.min(), y.max(), 13)  # 13 edges = 12 bins\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(y_train, bins=fixed_bins, kde=True, color='blue', stat='density')\n",
    "plt.title('MPG Distribution - Training Set')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(y_test, bins=fixed_bins, kde=True, color='green', stat='density')\n",
    "plt.title('MPG Distribution - Testing Set')\n",
    "plt.xlabel('Miles Per Gallon (MPG)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(5, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare statistical measures to verify balance\n",
    "print(\"\\nTraining set MPG statistics:\")\n",
    "print(y_train.describe())\n",
    "print(\"\\nTesting set MPG statistics:\")\n",
    "print(y_test.describe())\n",
    "\n",
    "# Calculate distribution similarity score between original and test set\n",
    "similarity_score = compare_distributions(y, y_test, \"Final Split\")\n",
    "print(f\"\\nFinal test set similarity score: {similarity_score['composite_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad728b",
   "metadata": {},
   "source": [
    "### 4.2 Train model using Scikit-Learn model.fit() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Train model using Scikit-Learn's Linear Regression\n",
    "\n",
    "# Initialize the linear regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "start_time = time.time()\n",
    "lr_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Model trained in {training_time:.4f} seconds\")\n",
    "\n",
    "# Get the coefficients and intercept\n",
    "print(\"\\nModel Parameters:\")\n",
    "print(f\"Intercept: {lr_model.intercept_:.4f}\")\n",
    "\n",
    "# Display some of the most influential coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "})\n",
    "\n",
    "# Sort coefficients by absolute value (to find most influential features)\n",
    "coefficients['Abs_Coefficient'] = np.abs(coefficients['Coefficient'])\n",
    "sorted_coeffs = coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most influential features:\")\n",
    "display(sorted_coeffs.head(10))\n",
    "\n",
    "# Make predictions on the training and test sets\n",
    "y_train_pred = lr_model.predict(X_train)\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Create a dataframe to store actual vs predicted values for the test set\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_test_pred,\n",
    "    'Residual': y_test - y_test_pred\n",
    "})\n",
    "\n",
    "# Display the first few rows of predictions vs. actual values\n",
    "print(\"\\nSample of Actual vs. Predicted values (Test Set):\")\n",
    "display(results_df.head(10))\n",
    "\n",
    "# Plot actual vs predicted for test set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual MPG')\n",
    "plt.ylabel('Predicted MPG')\n",
    "plt.title('Actual vs. Predicted MPG (Test Set)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd7c61",
   "metadata": {},
   "source": [
    "### 4.3 Evalulate performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df4b8c",
   "metadata": {},
   "source": [
    "#### 4.3.1 Regression: R^2, MAE, RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6185e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.1 Evaluate performance with R^2, MAE, and RMSE\n",
    "\n",
    "# Function to calculate and display performance metrics\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    # Median Absolute Error\n",
    "    med_ae = np.median(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n--- {dataset_name} Metrics ---\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "    print(f\"Median Absolute Error: {med_ae:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'R2': r2,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'MedianAE': med_ae\n",
    "    }\n",
    "\n",
    "# Evaluate on training set\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, \"Training Set\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, \"Test Set\")\n",
    "\n",
    "# Compare training vs test metrics to check for overfitting\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Training': [train_metrics['R2'], train_metrics['MAE'], train_metrics['RMSE']],\n",
    "    'Testing': [test_metrics['R2'], test_metrics['MAE'], test_metrics['RMSE']]\n",
    "}, index=['R² Score', 'Mean Absolute Error (MAE)', 'Root Mean Squared Error (RMSE)'])\n",
    "\n",
    "print(\"\\n--- Training vs Testing Performance ---\")\n",
    "display(metrics_comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a1de3",
   "metadata": {},
   "source": [
    "### Reflection 4: How well did the model perform? Any surprises in the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9984d2",
   "metadata": {},
   "source": [
    "## Section 5. Improve the Model or Try Alternates (Implement Pipelines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1636c",
   "metadata": {},
   "source": [
    "### 5.1 Implement Pipeline 1: Imputer → StandardScaler → Linear Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e835ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Implement Pipeline 1: Imputer → StandardScaler → Linear Regression\n",
    "\n",
    "print(\"### 5.1 Pipeline 1: Imputer → StandardScaler → Linear Regression ###\")\n",
    "\n",
    "# Create a pipeline with imputer, scaler, and linear regression\n",
    "pipeline1 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "start_time = time.time()\n",
    "pipeline1.fit(X_train, y_train)\n",
    "pipeline1_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Pipeline 1 trained in {pipeline1_training_time:.4f} seconds\")\n",
    "\n",
    "# Make predictions on training and test sets\n",
    "y_train_pred_pipeline1 = pipeline1.predict(X_train)\n",
    "y_test_pred_pipeline1 = pipeline1.predict(X_test)\n",
    "\n",
    "# Evaluate pipeline1 performance\n",
    "train_metrics_pipeline1 = evaluate_model(y_train, y_train_pred_pipeline1, \"Pipeline 1 (Training Set)\")\n",
    "test_metrics_pipeline1 = evaluate_model(y_test, y_test_pred_pipeline1, \"Pipeline 1 (Test Set)\")\n",
    "\n",
    "# Get the linear regression model from the pipeline\n",
    "linear_model = pipeline1.named_steps['regressor']\n",
    "\n",
    "# Display some information about the model\n",
    "print(\"\\nPipeline 1 Model Parameters:\")\n",
    "print(f\"Intercept: {linear_model.intercept_:.4f}\")\n",
    "\n",
    "# Display top coefficients (note: these are scaled coefficients now)\n",
    "coefficients_pipeline1 = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': linear_model.coef_\n",
    "})\n",
    "\n",
    "# Sort coefficients by absolute value\n",
    "coefficients_pipeline1['Abs_Coefficient'] = np.abs(coefficients_pipeline1['Coefficient'])\n",
    "sorted_coeffs_pipeline1 = coefficients_pipeline1.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most influential features (after scaling):\")\n",
    "display(sorted_coeffs_pipeline1.head(10))\n",
    "\n",
    "# Visualize actual vs predicted for test set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_test_pred_pipeline1, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual MPG')\n",
    "plt.ylabel('Predicted MPG')\n",
    "plt.title('Pipeline 1: Actual vs. Predicted MPG (Test Set)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe to store actual vs predicted values for the test set\n",
    "results_df_pipeline1 = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_test_pred_pipeline1,\n",
    "    'Residual': y_test - y_test_pred_pipeline1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695e332",
   "metadata": {},
   "source": [
    "### 5.2 Implement Pipeline 2: Imputer → Polynomial Features (degree=3) → StandardScaler → Linear Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab959fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Implement Pipeline 2: Imputer → Polynomial Features (degree=3) → StandardScaler → Linear Regression\n",
    "\n",
    "print(\"\\n### 5.2 Pipeline 2: Imputer → Polynomial Features (degree=3) → StandardScaler → Linear Regression ###\")\n",
    "\n",
    "# Create a pipeline with imputer, polynomial features, scaler, and linear regression\n",
    "pipeline2 = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Use all features including make dummies\n",
    "# No need to restrict to only numerical features since you mentioned your processor can handle it\n",
    "X_train_full = X_train.copy()  # Use all features\n",
    "X_test_full = X_test.copy()    # Use all features\n",
    "\n",
    "# Print feature count to understand the scale\n",
    "print(f\"Number of input features: {X_train_full.shape[1]}\")\n",
    "print(f\"Expected polynomial features (degree=3): {int((X_train_full.shape[1] + 3) * (X_train_full.shape[1] + 2) * (X_train_full.shape[1] + 1) / 6) - 1}\")\n",
    "\n",
    "# Train the pipeline on the training data with all features\n",
    "start_time = time.time()\n",
    "pipeline2.fit(X_train_full, y_train)\n",
    "pipeline2_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"Pipeline 2 trained in {pipeline2_training_time:.4f} seconds\")\n",
    "\n",
    "# Make predictions on training and test sets\n",
    "y_train_pred_pipeline2 = pipeline2.predict(X_train_full)\n",
    "y_test_pred_pipeline2 = pipeline2.predict(X_test_full)\n",
    "\n",
    "# Evaluate pipeline2 performance\n",
    "train_metrics_pipeline2 = evaluate_model(y_train, y_train_pred_pipeline2, \"Pipeline 2 (Training Set)\")\n",
    "test_metrics_pipeline2 = evaluate_model(y_test, y_test_pred_pipeline2, \"Pipeline 2 (Test Set)\")\n",
    "\n",
    "# Get all feature names for reference\n",
    "# Since we're using all features, we need to get all column names\n",
    "feature_names = X_train_full.columns.tolist()\n",
    "\n",
    "# Get the polynomial feature names\n",
    "poly_features = pipeline2.named_steps['poly'].get_feature_names_out(feature_names)\n",
    "\n",
    "# Get the linear regression model from the pipeline\n",
    "linear_model2 = pipeline2.named_steps['regressor']\n",
    "\n",
    "# Display model information\n",
    "print(\"\\nPipeline 2 Model Parameters:\")\n",
    "print(f\"Intercept: {linear_model2.intercept_:.4f}\")\n",
    "print(f\"Number of features after polynomial transformation: {len(poly_features)}\")\n",
    "\n",
    "# Display top coefficients\n",
    "coefficients_pipeline2 = pd.DataFrame({\n",
    "    'Feature': poly_features,\n",
    "    'Coefficient': linear_model2.coef_\n",
    "})\n",
    "\n",
    "# Sort coefficients by absolute value\n",
    "coefficients_pipeline2['Abs_Coefficient'] = np.abs(coefficients_pipeline2['Coefficient'])\n",
    "sorted_coeffs_pipeline2 = coefficients_pipeline2.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most influential polynomial features (after scaling):\")\n",
    "display(sorted_coeffs_pipeline2.head(10))\n",
    "\n",
    "# Visualize actual vs predicted for test set\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_test_pred_pipeline2, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual MPG')\n",
    "plt.ylabel('Predicted MPG')\n",
    "plt.title('Pipeline 2: Actual vs. Predicted MPG (Test Set)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe to store actual vs predicted values for the test set\n",
    "results_df_pipeline2 = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_test_pred_pipeline2,\n",
    "    'Residual': y_test - y_test_pred_pipeline2\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820a098",
   "metadata": {},
   "source": [
    "### 5.3 Compare performance of all models across the same performance metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Compare performance of all models across the same performance metrics\n",
    "\n",
    "print(\"\\n### 5.3 Compare Performance of All Models ###\")\n",
    "\n",
    "# Create a comparison dataframe for test metrics\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Baseline Linear Regression': [test_metrics['R2'], test_metrics['MAE'], test_metrics['RMSE'], test_metrics['MAPE']],\n",
    "    'Pipeline 1 (Scaling)': [test_metrics_pipeline1['R2'], test_metrics_pipeline1['MAE'], test_metrics_pipeline1['RMSE'], test_metrics_pipeline1['MAPE']],\n",
    "    'Pipeline 2 (Polynomial)': [test_metrics_pipeline2['R2'], test_metrics_pipeline2['MAE'], test_metrics_pipeline2['RMSE'], test_metrics_pipeline2['MAPE']]\n",
    "}, index=['R² Score', 'Mean Absolute Error (MAE)', 'Root Mean Squared Error (RMSE)', 'Mean Absolute Percentage Error (MAPE)'])\n",
    "\n",
    "print(\"\\n--- Test Set Performance Comparison ---\")\n",
    "display(models_comparison)\n",
    "\n",
    "# Training time comparison\n",
    "training_times = pd.DataFrame({\n",
    "    'Model': ['Baseline Linear Regression', 'Pipeline 1 (Scaling)', 'Pipeline 2 (Polynomial)'],\n",
    "    'Training Time (seconds)': [training_time, pipeline1_training_time, pipeline2_training_time]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Training Time Comparison ---\")\n",
    "display(training_times)\n",
    "\n",
    "# Visualize performance comparison - fixed version\n",
    "metrics_to_plot = ['R² Score', 'Mean Absolute Error (MAE)', 'Root Mean Squared Error (RMSE)']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot each metric\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = models_comparison.loc[metric].values\n",
    "    model_names = models_comparison.columns\n",
    "    \n",
    "    # For R², higher is better\n",
    "    if metric == 'R² Score':\n",
    "        axes[i].bar(range(len(model_names)), values)\n",
    "        axes[i].set_title(f'Comparison of {metric}\\n(higher is better)')\n",
    "    # For error metrics, lower is better\n",
    "    else:\n",
    "        axes[i].bar(range(len(model_names)), values)\n",
    "        axes[i].set_title(f'Comparison of {metric}\\n(lower is better)')\n",
    "    \n",
    "    axes[i].set_xticks(range(len(model_names)))\n",
    "    axes[i].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for j, v in enumerate(values):\n",
    "        axes[i].text(j, v + (0.01 if metric == 'R² Score' else 0.05), \n",
    "                    f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot of all models predictions vs actual values\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Base model\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual MPG')\n",
    "plt.ylabel('Predicted MPG')\n",
    "plt.title(f'Baseline Linear Regression\\nR² = {test_metrics[\"R2\"]:.3f}, RMSE = {test_metrics[\"RMSE\"]:.3f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Pipeline 1\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(y_test, y_test_pred_pipeline1, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual MPG')\n",
    "plt.ylabel('Predicted MPG')\n",
    "plt.title(f'Pipeline 1 (Scaling)\\nR² = {test_metrics_pipeline1[\"R2\"]:.3f}, RMSE = {test_metrics_pipeline1[\"RMSE\"]:.3f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Pipeline 2\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(y_test, y_test_pred_pipeline2, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual MPG')\n",
    "plt.ylabel('Predicted MPG')\n",
    "plt.title(f'Pipeline 2 (Polynomial)\\nR² = {test_metrics_pipeline2[\"R2\"]:.3f}, RMSE = {test_metrics_pipeline2[\"RMSE\"]:.3f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286f85e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fff3479",
   "metadata": {},
   "source": [
    "#### 5.3.1 Polynomial optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3.1 Optimizing Polynomial Degree (Testing degrees 3, 4, 6, and 9)\n",
    "\n",
    "print(\"\\n### 5.3.1 Optimizing Polynomial Degree ###\")\n",
    "\n",
    "# Degrees to test\n",
    "poly_degrees = [3, 4, 5, 7]\n",
    "\n",
    "# Dictionary to store results for each degree\n",
    "poly_results = {}\n",
    "poly_train_metrics = {}\n",
    "poly_test_metrics = {}\n",
    "poly_training_times = {}\n",
    "poly_predictions = {}\n",
    "\n",
    "# Only use numerical features to avoid feature explosion\n",
    "X_train_numeric = X_train[['weight', 'model_year']]\n",
    "X_test_numeric = X_test[['weight', 'model_year']]\n",
    "\n",
    "# Test each polynomial degree\n",
    "for degree in poly_degrees:\n",
    "    print(f\"\\nTesting Polynomial Degree {degree}\")\n",
    "    \n",
    "    # Create pipeline with the current degree\n",
    "    poly_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),  # Important to scale after polynomial transformation\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Train the pipeline\n",
    "    start_time = time.time()\n",
    "    poly_pipeline.fit(X_train_numeric, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    poly_training_times[degree] = train_time\n",
    "    \n",
    "    print(f\"Training time: {train_time:.4f} seconds\")\n",
    "    \n",
    "    # Get the polynomial feature names\n",
    "    poly_features = poly_pipeline.named_steps['poly'].get_feature_names_out(['weight', 'model_year'])\n",
    "    print(f\"Number of features after polynomial transformation: {len(poly_features)}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = poly_pipeline.predict(X_train_numeric)\n",
    "    y_test_pred = poly_pipeline.predict(X_test_numeric)\n",
    "    \n",
    "    # Store predictions for later visualization\n",
    "    poly_predictions[degree] = {\n",
    "        'train': y_train_pred,\n",
    "        'test': y_test_pred\n",
    "    }\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_metrics = evaluate_model(y_train, y_train_pred, f\"Degree {degree} (Training Set)\")\n",
    "    test_metrics = evaluate_model(y_test, y_test_pred, f\"Degree {degree} (Test Set)\")\n",
    "    \n",
    "    # Store results\n",
    "    poly_results[degree] = poly_pipeline\n",
    "    poly_train_metrics[degree] = train_metrics\n",
    "    poly_test_metrics[degree] = test_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebd659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison dataframe of test metrics\n",
    "poly_comparison = pd.DataFrame({\n",
    "    f'Degree {degree}': [\n",
    "        poly_test_metrics[degree]['R2'],\n",
    "        poly_test_metrics[degree]['MAE'],\n",
    "        poly_test_metrics[degree]['RMSE'],\n",
    "        poly_test_metrics[degree]['MAPE'],\n",
    "        poly_training_times[degree]\n",
    "    ] for degree in poly_degrees\n",
    "}, index=['R² Score', 'Mean Absolute Error (MAE)', 'Root Mean Squared Error (RMSE)', \n",
    "          'Mean Absolute Percentage Error (MAPE)', 'Training Time (seconds)'])\n",
    "\n",
    "print(\"\\n--- Polynomial Degree Comparison (Test Set Metrics) ---\")\n",
    "display(poly_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973dc7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Visualize metrics comparison\n",
    "metrics_to_plot = ['R² Score', 'Mean Absolute Error (MAE)', 'Root Mean Squared Error (RMSE)']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = poly_comparison.loc[metric].values\n",
    "    x_pos = range(len(poly_degrees))\n",
    "    \n",
    "    # For R², higher is better\n",
    "    if metric == 'R² Score':\n",
    "        axes[i].bar(x_pos, values, color='green')\n",
    "        axes[i].set_title(f'Comparison of {metric}\\n(higher is better)')\n",
    "    # For error metrics, lower is better\n",
    "    else:\n",
    "        axes[i].bar(x_pos, values, color='blue')\n",
    "        axes[i].set_title(f'Comparison of {metric}\\n(lower is better)')\n",
    "    \n",
    "    axes[i].set_xticks(x_pos)\n",
    "    axes[i].set_xticklabels([f'Degree {d}' for d in poly_degrees])\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for j, v in enumerate(values):\n",
    "        axes[i].text(j, v + (0.01 if metric == 'R² Score' else 0.05), \n",
    "                    f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot predictions vs actual values for all degrees\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, degree in enumerate(poly_degrees):\n",
    "    axes[i].scatter(y_test, poly_predictions[degree]['test'], alpha=0.7)\n",
    "    axes[i].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    axes[i].set_xlabel('Actual MPG')\n",
    "    axes[i].set_ylabel('Predicted MPG')\n",
    "    axes[i].set_title(f'Polynomial Degree {degree}\\nR² = {poly_test_metrics[degree][\"R2\"]:.3f}, RMSE = {poly_test_metrics[degree][\"RMSE\"]:.3f}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Check for signs of overfitting by comparing R² on training vs test sets\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_r2 = [poly_train_metrics[d]['R2'] for d in poly_degrees]\n",
    "test_r2 = [poly_test_metrics[d]['R2'] for d in poly_degrees]\n",
    "\n",
    "x = range(len(poly_degrees))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], train_r2, width, label='Training R²', color='green', alpha=0.7)\n",
    "plt.bar([i + width/2 for i in x], test_r2, width, label='Test R²', color='blue', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Training vs Test R² Score (Higher gap indicates potential overfitting)')\n",
    "plt.xticks(x, [f'{d}' for d in poly_degrees])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(train_r2):\n",
    "    plt.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "for i, v in enumerate(test_r2):\n",
    "    plt.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0175e23",
   "metadata": {},
   "source": [
    "### Reflection 5: Which models performed better? How does scaling impact results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a2e71",
   "metadata": {},
   "source": [
    "## Section 6. Final Thoughts & Insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67949b",
   "metadata": {},
   "source": [
    "### 6.1 Summarize findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93e137",
   "metadata": {},
   "source": [
    "### 6.2 Discuss challenges faced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414d6d8",
   "metadata": {},
   "source": [
    "### 6.3 If you had more time, what would you try next?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de30fa2",
   "metadata": {},
   "source": [
    "### Reflection 6: What did you learn from this project?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
